# -*- coding: utf-8 -*-
"""TM_Notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wff-qvoPnZx8UA4T_ssTTnN3hvgTWfvb

# Text Mining & Search
Authors
- Alexandre Crivellari
- Andrea Muscio


# Introduction

**Text mining** is a powerful and versatile technique for analyzing and interpreting large amounts of unstructured textual data. In the digital age, with the explosion of online content, the ability to efficiently analyze and understand text has become crucial across various fields, including politics, marketing, healthcare, and finance. Text mining enables the transformation of raw text into structured data, allowing for the identification of patterns, extraction of knowledge, and evidence-based decision-making.

In this project, we aim to explore and apply different text mining methodologies using the **LIAR dataset**. This dataset is particularly interesting and complex as it contains over 12,000 political statements made by public figures in the United States, each labeled as true, partially true, mostly-true, barely-true, half-true, false, pants-fire among other categories. Analyzing these statements can provide significant insights into political language and the dissemination of information, helping to identify trends in the truthfulness of public figures' statements.

The main tasks of this project are twofold:

1. **Topic Modeling**: This approach allows us to automatically discover the latent themes present in the corpus of statements. Topic modeling not only facilitates the understanding of the main topics discussed in the data but also helps to identify how different topics are distributed between true and false statements.

2. **Text Classification**: In this task, the goal is to build a model capable of distinguishing between true and false statements. Text classification is a crucial step in developing automated fact-checking systems, which can be used to combat misinformation and improve transparency in public discourse.

Using the LIAR dataset, this project will explore the effectiveness of topic modeling and text classification techniques in analyzing political statements, highlighting the potential of text mining in extracting valuable knowledge from complex textual data.

# Workspace setting

## Dependencies
"""

# Uncomment if needed to install the required packages

# %pip install pandas numpy matplotlib seaborn scikit-learn scipy
# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# %pip install captum
# %pip install gensim
# %pip install nltk gensim pyLDAvis
# %pip install wordcloud

"""## Libraries"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score
import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
import torch.nn.functional as F

# Set random seed for reproducibility
np.random.seed(42)
torch.manual_seed(42)

"""## Dataset upload and exploration"""

# Define the column names
columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info',
           'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts',
           'mostly_true_counts', 'pants_on_fire_counts', 'context']

# Read the TSV files
train_df = pd.read_csv('train.tsv', sep='\t', names=columns)
valid_df = pd.read_csv('valid.tsv', sep='\t', names=columns)
test_df = pd.read_csv('test.tsv', sep='\t', names=columns)

print(train_df.columns)

# Check for missing values
print(train_df.isnull().sum())

# Remove 'job_title' and 'state_info' columns from all datasets
columns_to_remove = ['job_title', 'state_info']

train_df = train_df.drop(columns=columns_to_remove)
valid_df = valid_df.drop(columns=columns_to_remove)
test_df = test_df.drop(columns=columns_to_remove)

# Verify the columns have been removed
print(train_df.columns)

# Check for missing values
print(train_df.isnull().sum())

# Handle missing values
categorical_columns = ['subject', 'speaker', 'party_affiliation', 'context']
numeric_columns = ['barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']

# For categorical columns, fill with 'Unknown' instead of 'unknown'
for col in categorical_columns:
    train_df[col] = train_df[col].fillna('Unknown')
    valid_df[col] = valid_df[col].fillna('Unknown')
    test_df[col] = test_df[col].fillna('Unknown')

# For numeric columns, ensure they are numeric and fill with median
for col in numeric_columns:
    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')
    valid_df[col] = pd.to_numeric(valid_df[col], errors='coerce')
    test_df[col] = pd.to_numeric(test_df[col], errors='coerce')

    median_value = train_df[col].median()
    train_df[col] = train_df[col].fillna(median_value)
    valid_df[col] = valid_df[col].fillna(median_value)
    test_df[col] = test_df[col].fillna(median_value)

# Define the custom order for the labels
label_order = ['true', 'mostly-true', 'half-true', 'barely-true', 'false', 'pants-fire']

# Convert 'label' column to a categorical type with the specified order
train_df['label'] = pd.Categorical(train_df['label'], categories=label_order, ordered=True)

train_df.head()

# Encode labels
le = LabelEncoder()
train_df['label_encoded'] = le.fit_transform(train_df['label'])
valid_df['label_encoded'] = le.transform(valid_df['label'])
test_df['label_encoded'] = le.transform(test_df['label'])

# Display class distribution
print(train_df['label'].value_counts(normalize=True))

# Define custom colors for the plot: green for true and mostly-true, light red for others
palette = {
    'true': 'lightgreen',
    'mostly-true': 'lightgreen',
    'half-true': 'lightcoral',
    'barely-true': 'lightcoral',
    'false': 'lightcoral',
    'pants-fire': 'lightcoral'
}

# Plot the class distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='label', data=train_df, order=label_order, palette=palette)
plt.title('Class Distribution in Training Data')
plt.xticks(rotation=45)
plt.show()

"""## Feature Engineering"""

# Create a feature for statement length
train_df['statement_length'] = train_df['statement'].str.len()
valid_df['statement_length'] = valid_df['statement'].str.len()
test_df['statement_length'] = test_df['statement'].str.len()

# Aggregate count features
count_columns = ['barely_true_counts', 'false_counts', 'half_true_counts',
                 'mostly_true_counts', 'pants_on_fire_counts']

train_df['total_statements'] = train_df[count_columns].sum(axis=1)
valid_df['total_statements'] = valid_df[count_columns].sum(axis=1)
test_df['total_statements'] = test_df[count_columns].sum(axis=1)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Binarization function for labels:
# 'true' and 'mostly-true' are considered True
# 'half-true', 'barely-true', 'false', and 'pants-fire' are considered False

def binarize_labels(label):
    if label in ['true', 'mostly-true']:
        return 'True'
    else:
        return 'False'

# Apply binary labeling to all datasets
train_df['binary_label'] = train_df['label'].apply(binarize_labels)
valid_df['binary_label'] = valid_df['label'].apply(binarize_labels)
test_df['binary_label'] = test_df['label'].apply(binarize_labels)

# Encode binary labels
le = LabelEncoder()
train_df['label_encoded'] = le.fit_transform(train_df['binary_label'])
valid_df['label_encoded'] = le.transform(valid_df['binary_label'])
test_df['label_encoded'] = le.transform(test_df['binary_label'])

# Display class distribution
print("Class distribution in training set:")
print(train_df['binary_label'].value_counts(normalize=True))

print("\nClass distribution in validation set:")
print(valid_df['binary_label'].value_counts(normalize=True))

print("\nClass distribution in test set:")
print(test_df['binary_label'].value_counts(normalize=True))

# Visualize class distribution
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.countplot(x='binary_label', data=pd.concat([train_df, valid_df, test_df]))
plt.title('Binary Class Distribution Across All Datasets')
plt.show()

# Print some sample rows to verify the transformation
print("\nSample rows from the training set:")
print(train_df[['statement', 'label', 'binary_label', 'label_encoded']].head(10))

# Save the label encoder
import joblib
joblib.dump(le, 'binary_label_encoder.joblib')
print("\nLabel Encoder saved as 'binary_label_encoder.joblib'")

"""## Wordcloud exploration

### Based on True/False Statement
"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Function to create and display a word cloud
def create_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=16)
    plt.show()

# Filter the dataset based on binary_label
true_text = ' '.join(train_df[train_df['binary_label'] == "True"]['statement'].astype(str))
false_text = ' '.join(train_df[train_df['binary_label'] == "False"]['statement'].astype(str))

# Generate word cloud for True statements
create_wordcloud(true_text, "Word Cloud for True Statements")

# Generate word cloud for False statements
create_wordcloud(false_text, "Word Cloud for False Statements")

"""### Based on Speaker"""

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Define the labels that are considered "True"
true_labels = ['true', 'mostly-true']

# Calculate the percentage of "True" statements for each speaker
def calculate_true_percentage(speaker, df):
    speaker_df = df[df['speaker'] == speaker]
    true_count = speaker_df[speaker_df['label'].isin(true_labels)].shape[0]
    total_count = speaker_df.shape[0]
    if total_count == 0:  # To avoid division by zero
        return 0
    return (true_count / total_count) * 100

# List of speakers
speakers = ['barack-obama', 'donald-trump', 'hillary-clinton', 'mitt-romney']

# Dictionary to store the percentage of True statements for each speaker
true_percentages = {speaker: calculate_true_percentage(speaker, train_df) for speaker in speakers}

# Function to create and display a word cloud with an annotation
def create_wordcloud_with_annotation(text, title, percentage):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    # Plot the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')

    # Add title and annotation
    plt.title(title, fontsize=16)
    annotation = f'Average True Statements: {percentage:.2f}%'
    plt.text(0.5, -0.1, annotation, fontsize=14, ha='center', va='center', transform=plt.gca().transAxes, color='blue')

    plt.show()

# Generate word clouds with annotations for each speaker
for speaker in speakers:
    # Filter statements by the current speaker
    speaker_statements = train_df[train_df['speaker'] == speaker]['statement'].astype(str)

    # Concatenate all statements into a single string
    speaker_text = ' '.join(speaker_statements)

    # Get the percentage of True statements
    percentage = true_percentages[speaker]

    # Generate and display the word cloud with annotation
    create_wordcloud_with_annotation(speaker_text, f'Word Cloud for {speaker.replace("-", " ").title()}', percentage)

"""# Task 1 - Text Classification (Binary)

### Traditional ML Technique - Random Forest Classifier
"""

# Text preprocessing function
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize
    tokens = nltk.word_tokenize(text)
    # Remove stopwords and lemmatize
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
    return ' '.join(cleaned_tokens)

# Apply text preprocessing
train_df['cleaned_statement'] = train_df['statement'].apply(preprocess_text)
valid_df['cleaned_statement'] = valid_df['statement'].apply(preprocess_text)
test_df['cleaned_statement'] = test_df['statement'].apply(preprocess_text)

# Prepare data for Traditional ML (Random Forest)
# TF-IDF Vectorization with a fixed number of features
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(train_df['cleaned_statement'])
X_valid_tfidf = tfidf.transform(valid_df['cleaned_statement'])
X_test_tfidf = tfidf.transform(test_df['cleaned_statement'])

# Prepare numerical features
numerical_features = ['statement_length', 'total_statements', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts']

scaler = StandardScaler()
X_train_num = scaler.fit_transform(train_df[numerical_features])
X_valid_num = scaler.transform(valid_df[numerical_features])
X_test_num = scaler.transform(test_df[numerical_features])

# Combine TF-IDF and numerical features
X_train = np.hstack((X_train_tfidf.toarray(), X_train_num))
X_valid = np.hstack((X_valid_tfidf.toarray(), X_valid_num))
X_test = np.hstack((X_test_tfidf.toarray(), X_test_num))

y_train = train_df['label_encoded']
y_valid = valid_df['label_encoded']
y_test = test_df['label_encoded']

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Train the model
def train_with_progress(X, y, n_estimators=100):
    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    rf_model.fit(X, y)
    return rf_model

print("Training Random Forest model...")
rf_model = train_with_progress(X_train_resampled, y_train_resampled)

# Evaluation function
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba)

    results = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC'],
        'Score': [accuracy, precision, recall, f1, auc_roc]
    })

    return results, y_pred, y_pred_proba

# Evaluate the model
print("Evaluating the model...")
results, y_pred, y_test_pred = evaluate_model(rf_model, X_test, y_test)
print(results)

# Print feature counts for debugging
print(f"Number of features in training data: {X_train_resampled.shape[1]}")
print(f"Number of features in test data: {X_test.shape[1]}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, confusion_matrix

# ROC Curve
def plot_roc_curve(y_true, y_pred_proba):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'AUC-ROC: {roc_auc_score(y_true, y_pred_proba):.2f}')
    plt.plot([0, 1], [0, 1], linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

# Confusion Matrix
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# Feature Importance
def plot_feature_importance(model, feature_names, top_n=20):
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    plt.figure(figsize=(12, 8))
    plt.title(f"Top {top_n} Feature Importances")
    plt.bar(range(top_n), importances[indices])
    plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=90)
    plt.tight_layout()
    plt.show()

# After model evaluation, add these lines:
print("\nGenerating performance visualizations...")

# Plot ROC curve
plot_roc_curve(y_test, y_test_pred)

# Plot confusion matrix
plot_confusion_matrix(y_test, y_pred)

# Plot feature importance
feature_names = tfidf.get_feature_names_out().tolist() + numerical_features
plot_feature_importance(rf_model, feature_names)

import joblib

def save_model(model, filename='random_forest_model.joblib'):
    joblib.dump(model, filename)
    print(f"Model saved as {filename}")

# Save the model
save_model(rf_model)

"""### Deep Learning Model - RoBERTa"""

# Read the TSV files
train_df = pd.read_csv('train.tsv', sep='\t', names=columns)
valid_df = pd.read_csv('valid.tsv', sep='\t', names=columns)
test_df = pd.read_csv('test.tsv', sep='\t', names=columns)

# Step 2: Preprocess the datasets for RoBERTa

def binarize_labels(label):
    if label in ['true', 'mostly-true']:
        return 'True'
    else:
        return 'False'

# Apply binary labeling to all datasets
train_df['binary_label'] = train_df['label'].apply(binarize_labels)
valid_df['binary_label'] = valid_df['label'].apply(binarize_labels)
test_df['binary_label'] = test_df['label'].apply(binarize_labels)

# Encode binary labels
le = LabelEncoder()
train_df['label_encoded'] = le.fit_transform(train_df['binary_label'])
valid_df['label_encoded'] = le.transform(valid_df['binary_label'])
test_df['label_encoded'] = le.transform(test_df['binary_label'])

# Load RoBERTa tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

# Tokenize and encode the statements
max_length = 128  # You can adjust this based on your data

def encode_statements(texts):
    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')

train_encodings = encode_statements(train_df['statement'].tolist())
valid_encodings = encode_statements(valid_df['statement'].tolist())
test_encodings = encode_statements(test_df['statement'].tolist())

# Create PyTorch datasets
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'],
                              torch.tensor(train_df['label_encoded'].tolist()))
valid_dataset = TensorDataset(valid_encodings['input_ids'], valid_encodings['attention_mask'],
                              torch.tensor(valid_df['label_encoded'].tolist()))
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'],
                             torch.tensor(test_df['label_encoded'].tolist()))

# Set device to CUDA if available, otherwise CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(train_df['label_encoded']), y=train_df['label_encoded'])
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

# Step 3: Train the model

# Set up the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).to(device)

# Set up the data loaders
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Set up the optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

import time

# Training loop
epochs = 3
for epoch in range(epochs):
    model.train()
    train_loss = 0
    start_time = time.time()

    # Create a tqdm progress bar for the training loader
    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Train]',
                      total=len(train_loader), unit='batch')

    for batch in train_pbar:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

        # Apply class weights to the loss
        loss = F.cross_entropy(outputs.logits, labels, weight=class_weights)

        train_loss += loss.item()
        loss.backward()
        optimizer.step()

        # Update the progress bar
        elapsed_time = time.time() - start_time
        batch_time = elapsed_time / (train_pbar.n + 1)
        remaining_time = batch_time * (len(train_loader) - train_pbar.n - 1)
        train_pbar.set_postfix({
            'loss': f'{train_loss / (train_pbar.n + 1):.4f}',
            'elapsed': f'{time.strftime("%H:%M:%S", time.gmtime(elapsed_time))}',
            'remaining': f'{time.strftime("%H:%M:%S", time.gmtime(remaining_time))}',
            'total': f'{time.strftime("%H:%M:%S", time.gmtime(elapsed_time + remaining_time))}'
        })

    train_pbar.close()

    # Validation
    model.eval()
    valid_loss = 0
    valid_pbar = tqdm(valid_loader, desc=f'Epoch {epoch + 1}/{epochs} [Valid]',
                      total=len(valid_loader), unit='batch')

    with torch.no_grad():
        for batch in valid_pbar:
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            valid_loss += outputs.loss.item()

            # Update the progress bar
            valid_pbar.set_postfix({'loss': f'{valid_loss / (valid_pbar.n + 1):.4f}'})

    valid_pbar.close()

    print(f'Epoch {epoch + 1}/{epochs}:')
    print(f'Train Loss: {train_loss / len(train_loader):.4f}')
    print(f'Validation Loss: {valid_loss / len(valid_loader):.4f}')
    print(f'Epoch Time: {time.strftime("%H:%M:%S", time.gmtime(time.time() - start_time))}')
    print()

print("Training completed!")

# Save the model
torch.save(model.state_dict(), 'roberta_binary_classifier.pt')
print("Model saved as 'roberta_binary_classifier.pt'")

import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# 4. Evaluate the model against the test set

def evaluate_model(model, data_loader, device):
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            predictions.extend(preds)
            true_labels.extend(labels.cpu().numpy())

    return np.array(predictions), np.array(true_labels)

# Load the saved model
model.load_state_dict(torch.load('roberta_binary_classifier.pt'))
model.to(device)

# Evaluate on test set
test_predictions, test_true_labels = evaluate_model(model, test_loader, device)

# Calculate metrics
accuracy = accuracy_score(test_true_labels, test_predictions)
precision, recall, f1, _ = precision_recall_fscore_support(test_true_labels, test_predictions, average='binary')
auc_roc = roc_auc_score(test_true_labels, test_predictions)

print(f"Test Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"AUC-ROC: {auc_roc:.4f}")

# 5. Produce graphs for showing model performance

# Confusion Matrix
cm = confusion_matrix(test_true_labels, test_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# ROC Curve
from sklearn.metrics import roc_curve

fpr, tpr, _ = roc_curve(test_true_labels, test_predictions)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_roc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

# Precision-Recall Curve
from sklearn.metrics import precision_recall_curve

precision_curve, recall_curve, _ = precision_recall_curve(test_true_labels, test_predictions)
plt.figure(figsize=(8, 6))
plt.plot(recall_curve, precision_curve, label='Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# Class Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x=test_true_labels)
plt.title('Class Distribution in Test Set')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# Save the evaluation results
results = {
    'accuracy': accuracy,
    'precision': precision,
    'recall': recall,
    'f1_score': f1,
    'auc_roc': auc_roc
}

def evaluate_model(model, data_loader, device):
    model.eval()
    all_predictions = []
    all_true_labels = []
    all_probabilities = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids, attention_mask, labels = [b.to(device) for b in batch]
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_predictions.extend(preds)
            all_true_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities[:, 1].cpu().numpy())

    y_pred = np.array(all_predictions)
    y_test = np.array(all_true_labels)
    y_pred_proba = np.array(all_probabilities)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba)

    results = pd.DataFrame({
        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC'],
        'Score': [accuracy, precision, recall, f1, auc_roc]
    })

    return results, y_pred, y_pred_proba

# Load the saved model
model.load_state_dict(torch.load('roberta_binary_classifier.pt'))
model.to(device)

# Evaluate the model
print("Evaluating the RoBERTa model...")
results, y_pred, y_test_pred = evaluate_model(model, test_loader, device)
print(results)

"""### Model Comparison

Notes on why Roberta performs worse:

Dataset size: If the dataset is relatively small, traditional ML models like Random Forest can sometimes perform better than deep learning models, which typically require larger datasets to shine.

Feature engineering: The Random Forest model benefits from manually engineered features, which might capture important aspects of the data that the RoBERTa model hasn't learned to identify.

Task simplicity: If the classification task is relatively simple and doesn't require deep understanding of language nuances, a simpler model like Random Forest might be sufficient.

Hyperparameter tuning: The Random Forest model might be better tuned for this specific task compared to the current RoBERTa implementation.

Overfitting: The RoBERTa model, being more complex, might be overfitting to the training data, especially if the dataset is small.
"""







"""# Task 2 - Topic Modelling

# Latent Dirichlet Allocation (LDA) in Topic Modeling

**Latent Dirichlet Allocation (LDA)** is a generative probabilistic model used for identifying the underlying topics in a collection of documents. In the context of topic modeling, LDA assumes that each document is a mixture of various topics, and each topic is characterized by a distribution over words. The goal of LDA is to discover the hidden thematic structure in the corpus and to assign each word in a document to one of the topics.

## How LDA Works

LDA operates under the assumption that documents are generated through a probabilistic process:

1. **Topic Distribution for Each Document**:
    - For each document in the corpus, LDA assumes a Dirichlet distribution over a fixed number of topics. This distribution is represented by a vector $\theta_d$, where $\theta_d$ indicates the proportion of each topic in the document $d$.
    
2. **Word Distribution for Each Topic**:
    - Each topic $z$ is represented by a multinomial distribution over words, denoted as $\phi_z$. This distribution is also drawn from a Dirichlet prior, indicating the probability of each word in the vocabulary appearing in the topic.

3. **Document Generation Process**:
    - For each word in a document, LDA first selects a topic $z$ according to the topic distribution $\theta_d$ for that document. Then, it generates the word by sampling from the word distribution $\phi_z$ corresponding to the chosen topic.

4. **Inference**:
    - The challenge in LDA is to reverse this generative process: given a collection of documents, the model infers the most likely set of topics, the topic distribution for each document, and the word distribution for each topic. This is typically done using approximation techniques like Variational Inference or Gibbs Sampling.
    
## Applying LDA to Topic Modeling

When applied to topic modeling, LDA helps in discovering the hidden thematic structure in a large corpus of text. Here's how LDA can be utilized:

- **Identifying Topics**:
    - By running LDA on a set of documents, we can uncover a predefined number of topics, where each topic is a distribution over words. For instance, in a collection of news articles, LDA might identify topics such as "politics," "technology," or "health," each represented by a set of keywords that frequently co-occur in the context of that topic.

- **Document Representation**:
    - After fitting the LDA model, each document in the corpus can be represented as a mixture of these topics. For example, an article might be 70% about "politics" and 30% about "health," providing a more nuanced understanding of the content than just a single label.

- **Topic Interpretation**:
    - The output of LDA includes the top words associated with each topic, which can be interpreted by a human to assign labels or categories to the topics. This interpretation is crucial for understanding the dominant themes in the corpus and how different topics are distributed across documents.

## Benefits of Using LDA

- **Unsupervised Learning**:
    - LDA is an unsupervised learning algorithm, meaning it does not require labeled data. It automatically discovers topics within the text, making it a powerful tool for exploring and understanding large, unstructured datasets.

- **Scalability**:
    - LDA is scalable and can handle large corpora, making it suitable for applications ranging from social media analysis to news categorization and beyond.

- **Flexibility**:
    - The model's flexibility allows it to capture a wide range of topics in diverse domains, making it applicable in various fields such as marketing, politics, and literature analysis.


"""

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import gensim
from gensim import corpora
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis
import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""## Dataset preprocessing for LDA"""

# Display the first few rows
print(train_df.head())

# Focus on the 'statement' column for topic modeling
statements = train_df['statement'].astype(str)

# Initialize the stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Function to preprocess text
def preprocess_text(text):
    # Tokenize the text
    words = word_tokenize(text.lower())
    # Remove stopwords and punctuation, and lemmatize the words
    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]
    return words

# Apply preprocessing to all statements
processed_statements = statements.apply(preprocess_text)

# View the processed text
print(processed_statements.head())

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary(processed_statements)

# Filter out extreme values in the dictionary (optional)
dictionary.filter_extremes(no_below=15, no_above=0.5)

# Create a Bag-of-Words corpus
corpus = [dictionary.doc2bow(text) for text in processed_statements]

# Display a sample of the corpus
print(corpus[:2])

"""## Generic study

### Topic Modeling
"""

# Set the number of topics
num_topics = 5

# Build the LDA model
lda_model = gensim.models.LdaModel(corpus=corpus,
                                   id2word=dictionary,
                                   num_topics=num_topics,
                                   random_state=42,
                                   update_every=1,
                                   chunksize=100,
                                   passes=10,
                                   alpha='auto',
                                   per_word_topics=True)

# Display the topics
topics = lda_model.print_topics(num_words=10)
for topic in topics:
    print(topic)

"""### Topics Visulization"""

# Visualize the topics using pyLDAvis
lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(lda_vis)

# Save the visualization to an HTML file (optional)
pyLDAvis.save_html(lda_vis, 'lda_topics.html')

"""[HERE LINK TO HTML PAGE]"""

# Assign the most likely topic to each document
def assign_topic(lda_model, corpus):
    topics = []
    for bow in corpus:
        topic_distribution = lda_model.get_document_topics(bow)
        topics.append(sorted(topic_distribution, key=lambda x: x[1], reverse=True)[0][0])
    return topics

train_df['topic'] = assign_topic(lda_model, corpus)
print(train_df[['statement', 'topic']].head())

# Save the dataframe with the assigned topics
train_df.to_csv('train_with_topics.csv', index=False)

"""### Evaluation

## Perplexity and Coherence in Topic Modeling

### Perplexity

**Perplexity** is a measure of how well a probabilistic model predicts a sample. In the context of topic modeling, perplexity is used to evaluate the quality of the LDA model by estimating how accurately the model can predict unseen data. It is calculated as the inverse of the geometric mean of the likelihood of a test set, given the model. A lower perplexity score indicates a better generalization performance of the model.

### Coherence

**Coherence** measures the interpretability of the topics generated by a model. It evaluates how semantically consistent the top words within each topic are. Coherence is typically calculated based on the degree of semantic similarity between words in the topic. Higher coherence scores indicate that the topics are more interpretable and meaningful, making them more useful for practical applications.

#### Perplexity
"""

# Calculate the perplexity score
log_perplexity = lda_model.log_perplexity(corpus)
perplexity = np.exp(log_perplexity)
print(f'Perplexity: {perplexity}')

"""#### Coherence"""

from gensim.models import CoherenceModel

# Calculate coherence score
coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_statements, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print(f'Coherence Score: {coherence_lda}')

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.LdaModel(corpus=corpus,
                                       id2word=dictionary,
                                       num_topics=num_topics,
                                       random_state=42,
                                       update_every=1,
                                       chunksize=100,
                                       passes=10,
                                       alpha='auto',
                                       per_word_topics=True)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
    return model_list, coherence_values

# Run the function over a range of topics
start, limit, step = 2, 10, 1
model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_statements, start=start, limit=limit, step=step)

# Plot coherence values
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("Number of Topics")
plt.ylabel("Coherence Score")
plt.title("Coherence Score vs. Number of Topics")
plt.show()

"""## Topics between True and False Statements

### Preprocessing
"""

true_labels = ['true', 'mostly-true']
false_labels = ['false', 'pants-fire', 'barely-true']

# Filter the dataset for True statements
true_statements = train_df[train_df['label'].isin(true_labels)]['statement'].astype(str)

# Filter the dataset for False statements
false_statements = train_df[train_df['label'].isin(false_labels)]['statement'].astype(str)

# Reuse the preprocess_text function from earlier

# Preprocess the True statements
processed_true_statements = true_statements.apply(preprocess_text)

# Preprocess the False statements
processed_false_statements = false_statements.apply(preprocess_text)

# Create a dictionary and corpus for True statements
true_dictionary = corpora.Dictionary(processed_true_statements)
true_dictionary.filter_extremes(no_below=15, no_above=0.5)
true_corpus = [true_dictionary.doc2bow(text) for text in processed_true_statements]

# Create a dictionary and corpus for False statements
false_dictionary = corpora.Dictionary(processed_false_statements)
false_dictionary.filter_extremes(no_below=15, no_above=0.5)
false_corpus = [false_dictionary.doc2bow(text) for text in processed_false_statements]

"""### Topic Modeling"""

# Set the number of topics
num_topics = 5

# Build the LDA model for True statements
lda_model_true = gensim.models.LdaModel(corpus=true_corpus,
                                        id2word=true_dictionary,
                                        num_topics=num_topics,
                                        random_state=42,
                                        update_every=1,
                                        chunksize=100,
                                        passes=10,
                                        alpha='auto',
                                        per_word_topics=True)

# Build the LDA model for False statements
lda_model_false = gensim.models.LdaModel(corpus=false_corpus,
                                         id2word=false_dictionary,
                                         num_topics=num_topics,
                                         random_state=42,
                                         update_every=1,
                                         chunksize=100,
                                         passes=10,
                                         alpha='auto',
                                         per_word_topics=True)

"""### Topics Visualization"""

# Display topics for True statements
print("Topics for True statements:")
true_topics = lda_model_true.print_topics(num_words=10)
for topic in true_topics:
    print(topic)

# Display topics for False statements
print("\nTopics for False statements:")
false_topics = lda_model_false.print_topics(num_words=10)
for topic in false_topics:
    print(topic)

"""#### Topics in True statements"""

# Visualize True statement topics
lda_vis_true = gensimvis.prepare(lda_model_true, true_corpus, true_dictionary)
pyLDAvis.display(lda_vis_true)

"""#### Topics in False statements"""

# Visualize False statement topics
lda_vis_false = gensimvis.prepare(lda_model_false, false_corpus, false_dictionary)
pyLDAvis.display(lda_vis_false)

"""## Evaluation

### Perplexity
"""

# Perplexity for the True statements model
perplexity_true = lda_model_true.log_perplexity(true_corpus)
print(f'Log Perplexity for True statements: {perplexity_true}')

# Perplexity for the False statements model
perplexity_false = lda_model_false.log_perplexity(false_corpus)
print(f'Log Perplexity for False statements: {perplexity_false}')

# To convert log perplexity to perplexity:
perplexity_true_exp = np.exp(perplexity_true)
perplexity_false_exp = np.exp(perplexity_false)
print(f'Perplexity for True statements: {perplexity_true_exp}')
print(f'Perplexity for False statements: {perplexity_false_exp}')

"""### Coherence"""

from gensim.models import CoherenceModel

# Coherence score for True statements
coherence_model_true = CoherenceModel(model=lda_model_true, texts=processed_true_statements, dictionary=true_dictionary, coherence='c_v')
coherence_true = coherence_model_true.get_coherence()
print(f'Coherence Score for True statements: {coherence_true}')

# Coherence score for False statements
coherence_model_false = CoherenceModel(model=lda_model_false, texts=processed_false_statements, dictionary=false_dictionary, coherence='c_v')
coherence_false = coherence_model_false.get_coherence()
print(f'Coherence Score for False statements: {coherence_false}')